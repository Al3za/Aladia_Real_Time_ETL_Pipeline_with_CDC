import os
from urllib.parse import urlparse
from dotenv import load_dotenv
# Creare Spark session con supporto Kafka:
from pyspark.sql import SparkSession

# Carica le variabili dal file
#load_dotenv("config/postgres.env")
load_dotenv()

db_url = os.environ["DATABASE_URL"]

parsed = urlparse(db_url)
port = parsed.port or 5432
jdbc_url = f"jdbc:postgresql://{parsed.hostname}:{port}{parsed.path}?sslmode=require" # jdbc_url → Spark sa dove connettersi
jdbc_properties = { # jdbc_properties → contiene user/password/driver
    "user": parsed.username,
    "password": parsed.password,
    "driver": "org.postgresql.Driver"
}

print("JDBC URL:", jdbc_url)


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
JARS_DIR = os.path.join(BASE_DIR, "jars")
print(JARS_DIR, 'JARS_DIR here')

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("CDC_Employee")
    .master("local[*]")
    .config(
        "spark.jars",
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/kafka-clients-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-token-provider-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/commons-pool2-2.11.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/postgresql-42.6.0.jar"
    )
    .getOrCreate()
) 


spark.sparkContext.setLogLevel("WARN")
print("Spark ready! Version:", spark.version)


# Leggere dal topic di kafka. ricorda che i topik sono i messaggi CDC salvati qui in kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:29092") \
    .option("subscribe", "cdc.public.Employee") \
    .option("startingOffsets", "earliest") \
    .load()

#  HERE STARTS ETL: 
from pyspark.sql.functions import from_json, col, to_timestamp, expr, lit
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Estarai solo i dati che ci servono da payload
schema = StructType([
    StructField("payload", StructType([ # res #payload" (nel message kafka)
        StructField("op", StringType(), True), # in op is store: insert/update/delete operation type in topyc generated by Debezium
        StructField("after", StructType([ # valori nuovi(insert) ho aggiornati(update)
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True),
         StructField("before", StructType([      # valori prima dell'update o delete
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True)
    ]), True),
     StructField("_corrupt_record", StringType(), True)
]) # same types as the sink db table, to avoid errors


# parse the "payload" message data stored in kafka.

df_parsed = (
    df.selectExpr("CAST(value AS STRING) as json_str")
      .select(from_json(
              col("json_str"), 
               schema,
                {"mode": "PERMISSIVE", 
                 "columnNameOfCorruptRecord": "_corrupt_record"})
                .alias("data"))
) #"PERMISSIVE": Spark salva i record malformati in _corrupt_record invece di far crashare il job spark.

# df_parsed.printSchema() # check lo schema

df_flat = df_parsed.select(
          col("data.payload.op").alias("op"), # qui attingiamo il campo "op" di debezium (insert/update/delete)
          col("data.payload.after.*"), # # valori post-update/insert (di kafka)
          col("data.payload.before.id").alias("before_id"), # valori pre-update/delete )
          col("data._corrupt_record")  # qui finiranno i record json malformati
) # Le colonne di controllo CDC


# separate malformed e valid json records(found errors list in tmp/badRecords_checkpoint)
df_malformed = df_flat.filter(col("_corrupt_record").isNotNull()) # record json dei kafka topic malformati(_corrupt_record,andranno in tmp/badRecords_checkpoint, non andranno in db)
df_valid_json = df_flat.filter(col("_corrupt_record").isNull())


# DATA VALIDATION

valid_df = df_valid_json.filter( 
    (col("role").isin("ADMIN", "ENGINEER","INTERN")) &
    (col("email").isNotNull()) &
    (col("password")).isNotNull() &  
    (col("createdAt") > 0)
)

invalid_df = df_valid_json.filter(
    (~col("role").isin("ADMIN", "ENGINEER", "INTERN")) |
    col("email").isNull() |
    (col("createdAt") <= 0)
)

invalid_df = invalid_df.withColumn(
    "error_reason",
    expr("""
        CASE
            WHEN role NOT IN ('ADMIN','ENGINEER','INTERN') THEN 'INVALID_ROLE'
            WHEN email IS NULL THEN 'NULL_EMAIL'
            WHEN createdAt <= 0 THEN 'INVALID_CREATED_AT'
            ELSE 'UNKNOWN'
        END
    """)
) # this error will be shows this error if we investigate in /tmp/badRecords_checkpoint

(
    df_malformed
    .unionByName(invalid_df, allowMissingColumns=True) # unifica due categorie diverse di errore: errori json, e non allowed roles, datetimes...
    .writeStream
    .format("parquet")
    .option("path", "/tmp/badRecords")
    .option("checkpointLocation", "/tmp/badRecords_checkpoint")
    .outputMode("append")
    .start()
)


def write_to_postgres(batch_df, batch_id):

    if batch_df.isEmpty():
        return

    # INSERT / UPDATE (ADMIN ONLY)
  
    upsert_df = (
        batch_df
        .filter(col("op").isin("c", "u"))
        .filter(col("role") == "ADMIN")
        .withColumn("updated_ts", to_timestamp(col("updatedAt") / 1000)) # withColumn crea una colonna temporanea che possiamo usare come riferimento, come facciamo sotto
        .dropDuplicates(["id", "updated_ts"]) # si basa su payload "after" data.  
        # “Se Kafka mi manda due volte lo stesso evento CDC (stesso id + stesso timestamp), ne tengo solo uno.” (Perfetto per CDC Debezium, perche a volte crea lo stesso evento 2)
        # questo non conta per update, perche avremo lo stesso id, ma update_ts diverso
    )

    
        # DROP COLONNE CDC / TECNICHE PRIMA DEL WRITE. in valid_df abbiamo non solo i datatype del db (email,name..) ma anche le
        # colonne CDC definite in "schema/". upsert_df trasorta tutti questi nel blocco codice sotto, dove in pratica facciamo gli insert/update
        # spark le confronta questi dati con la tabella "employee_admin" ma la' non risulta la colonna "op","before_id".. causando l'error
        # quindi le droppiamo per lasciare solo le colonne uguali allo schema del sink db 
    upsert_df = upsert_df.drop("op", "before_id", "_corrupt_record", "updated_ts") # cosi' restano solo le colonne che spark 
        # va' ad inserire nel sink db

    if not upsert_df.isEmpty(): 
        # in upsert_df arrivano i dati di valid_df definiti in 'query'(sotto) quando chiamiamo
        #  questa funzione write_to_postgres
        upsert_df.write.jdbc( # JDBC = Java DataBase Connectivity. 
            # il meccanismo standard con cui Spark (che gira su JVM) parla con i database.(fa' operazioni molto velocemente)
            url=jdbc_url,
            table="employee_admin",
            mode="append",
            properties=jdbc_properties
        )

    # DELETE
    delete_ids = (
        batch_df
        .filter(col("op") == "d")
        .select("before_id")
        .dropna()
        .distinct()
        .collect()
    )

    if delete_ids:
        ids = [r.before_id for r in delete_ids]

        import psycopg2 # usiamo psycopg2 invece di JDBC perché Spark JDBC non ha le funzioni di DELETE 
        # ma solo insert/update
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        cur.execute(
            "DELETE FROM employee_admin WHERE id = ANY(%s);",
            (ids,)
        )
        conn.commit()
        cur.close()
        conn.close()

query = (
    valid_df # ad ogni nuovo inser/delete/update, passiamo questi dati e facciamo operazioni crud 
    # in write_to_postgres db
    .writeStream
    .foreachBatch(write_to_postgres)
    .option("checkpointLocation", "/tmp_spark/checkpoint")
    .start()
)

query.awaitTermination()

# rm -rf /tmp_spark/checkpoint