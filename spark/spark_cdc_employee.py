import os
from urllib.parse import urlparse
from dotenv import load_dotenv
# Creare Spark session con supporto Kafka:
from pyspark.sql import SparkSession
import psycopg2 

# Carica le variabili dal file
#load_dotenv("config/postgres.env")
load_dotenv()

db_url = os.environ["DATABASE_URL"]

parsed = urlparse(db_url)
port = parsed.port or 5432
jdbc_url = f"jdbc:postgresql://{parsed.hostname}:{port}{parsed.path}?sslmode=require" # jdbc_url → Spark sa dove connettersi
jdbc_properties = { # jdbc_properties → contiene user/password/driver
    "user": parsed.username,
    "password": parsed.password,
    "driver": "org.postgresql.Driver"
}

print("JDBC URL:", jdbc_url)


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
JARS_DIR = os.path.join(BASE_DIR, "jars")
print(JARS_DIR, 'JARS_DIR here')

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("CDC_Employee")
    .master("local[*]")
    .config(
        "spark.jars",
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/kafka-clients-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-token-provider-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/commons-pool2-2.11.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/postgresql-42.6.0.jar"
    )
    .getOrCreate()
) 


spark.sparkContext.setLogLevel("WARN")
print("Spark ready! Version:", spark.version)


# Leggere dal topic di kafka. ricorda che i topik sono i messaggi CDC salvati qui in kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:29092") \
    .option("subscribe", "cdc.public.Employee") \
    .option("startingOffsets", "earliest") \
    .load()

#  HERE STARTS ETL: 
from pyspark.sql.functions import from_json, col, to_timestamp, expr, lit
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Estarai solo i dati che ci servono da payload
schema = StructType([
    StructField("payload", StructType([ # res #payload" (nel message kafka)
        StructField("op", StringType(), True), # in op is store: insert/update/delete operation type in topyc generated by Debezium
        StructField("after", StructType([ # valori nuovi(insert) ho aggiornati(update)
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True),
         StructField("before", StructType([      # valori prima dell'update o delete
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True)
    ]), True),
     StructField("_corrupt_record", StringType(), True)
]) # same types as the sink db table, to avoid errors


# parse the "payload" message data stored in kafka.

df_parsed = ( # data cleaning
    df.selectExpr("CAST(value AS STRING) as json_str")
      .select(from_json(
              col("json_str"), 
               schema,
                {"mode": "PERMISSIVE", 
                 "columnNameOfCorruptRecord": "_corrupt_record"})
                .alias("data"))
) #"PERMISSIVE": Spark salva i record malformati in _corrupt_record invece di far crashare il job spark.

# df_parsed.printSchema() # check lo schema

df_flat = df_parsed.select(
          col("data.payload.op").alias("op"), # qui attingiamo il campo "op" di debezium (insert/update/delete)
          col("data.payload.after.*"), # # values post-update/insert (di kafka)
          col("data.payload.before.id").alias("before_id"), # values pre-update/delete )
          col("data._corrupt_record")  # here will land json invalid/malformed records
) # Le colonne di controllo CDC


# handle  malformed e valid json records(found errors list in tmp/badRecords_checkpoint)
df_malformed = df_flat.filter(col("_corrupt_record").isNotNull()) # record json dei kafka topic malformati(_corrupt_record,andranno in tmp/badRecords_checkpoint, non andranno in db)
df_valid_json = df_flat.filter(col("_corrupt_record").isNull())

# separate delete functionality to access it in write_to_postgres function(down below)
delete_df = df_valid_json.filter(col("op") == "d")

# validation only for create/update/read operations
non_delete_df = df_valid_json.filter(col("op").isin("c", "u", "r"))

# DATA VALIDATION

valid_df = non_delete_df.filter( 
    (col("role").isin("ADMIN", "ENGINEER","INTERN")) &
    (col("email").isNotNull()) &
    (col("password")).isNotNull() &  
    (col("createdAt") > 0)
)

print('valid_df schema:', valid_df)

# gracefully handle invalid records without blocking spark stream 
invalid_df = df_valid_json.filter(
    (~col("role").isin("ADMIN", "ENGINEER", "INTERN")) |
    col("email").isNull() |
    (col("createdAt") <= 0)
)

invalid_df = invalid_df.withColumn(
    "error_reason",
    expr("""
        CASE
            WHEN role NOT IN ('ADMIN','ENGINEER','INTERN') THEN 'INVALID_ROLE'
            WHEN email IS NULL THEN 'NULL_EMAIL'
            WHEN createdAt <= 0 THEN 'INVALID_CREATED_AT'
            ELSE 'UNKNOWN'
        END
    """)
) # this error will be shows this error if we investigate in /tmp/badRecords_checkpoint

# store malformed and invalid records in badRecords folder(for further analysis)
(
    df_malformed
    .unionByName(invalid_df, allowMissingColumns=True) # unifica due categorie diverse di errore: errori json, e non allowed roles, datetimes...
    .writeStream
    .format("parquet")
    .option("path", "/tmp/badRecords")
    .option("checkpointLocation", "/tmp/badRecords_checkpoint")
    .outputMode("append")
    .start()
)

# FUNCTION TO WRITE BATCH TO POSTGRESQL(with jdbc for insert/update and psycopg2 for delete)
def write_to_postgres(batch_df, batch_id):

    if batch_df.isEmpty():
        return # return nothing

    insert_df   = batch_df.filter(col("op") == "c")
    update_df   = batch_df.filter(col("op") == "u")
    snapshot_df = batch_df.filter(col("op") == "r")
    delete_df   = batch_df.filter(col("op") == "d") # works tnx to delete_df data  defined in cdc_df(delete_df): (cdc_df = valid_df.unionByName(delete_df, allowMissingColumns=True))
  
    upsert_source_df = (
        insert_df
        .unionByName(update_df)
        .unionByName(snapshot_df)
    ) 

    upsert_df = (
        upsert_source_df # batch dara with where applies business logic and deduplication
        .filter(col("role") == "ADMIN") # keep only ADMIN role records for insert/update
        .withColumn("updated_ts", to_timestamp(col("updatedAt") / 1000)) # withColumn crea una colonna temporanea che possiamo usare come riferimento, come facciamo sotto
        .dropDuplicates(["id", "updated_ts"]) # si basa su payload "after" data.  
        # “Se Kafka mi manda due volte lo stesso evento CDC (stesso id + stesso timestamp), ne tengo solo uno.” (Perfetto per CDC Debezium, perche a volte crea lo stesso evento 2)
        # questo non conta per update, perche avremo lo stesso id, ma update_ts diverso
    )
        # DROP COLONNE CDC / TECNICHE PRIMA DEL WRITE. in cdc_df abbiamo non solo i datatype del db (email,name..) ma anche le
        # colonne CDC definite in "schema/". upsert_df trasorta tutti questi nel blocco codice sotto, dove in pratica facciamo gli insert/update
        # spark le confronta questi dati con la tabella "employee_admin" ma la' non risulta la colonna "op","before_id".. causando l'error
        # quindi le droppiamo per lasciare solo le colonne uguali allo schema del sink db 
    upsert_df = upsert_df.drop("op", "before_id", "_corrupt_record", "updated_ts") # cosi' restano solo le colonne che spark 
        # va' ad inserire nel sink db
   
 
    if not upsert_df.isEmpty(): 
        # in upsert_df arrivano i dati di cdc_df definiti in 'query'(sotto) quando chiamiamo
        #  questa funzione write_to_postgres
        upsert_df.write.jdbc( # JDBC = Java DataBase Connectivity. 
            # il meccanismo standard con cui Spark (che gira su JVM) parla con i database.(fa' operazioni molto velocemente)
            url=jdbc_url,
            table="employee_admin", # Table created by Spark 
            mode="append", # append = add new records without delete the old ones
            properties=jdbc_properties
        )

    # 2️⃣ UPDATE
    # update_rows = (
    #     update_df
    #     .filter(col("role") == "ADMIN")
    #     .select(
    #         "id", "email", "name", "password", "role",
    #         to_timestamp(col("updatedAt") / 1000).alias("updated_ts")
    #     )
    #     .dropDuplicates(["id", "updated_ts"])
    #     .collect()
    # )

    # if update_rows:
    #     conn = psycopg2.connect(db_url)
    #     cur = conn.cursor()
    #     for r in update_rows:
    #         cur.execute(
    #             """
    #             UPDATE employee_admin
    #             SET email = %s,
    #                 name = %s,
    #                 password = %s,
    #                 role = %s,
    #                 updatedAt = %s
    #             WHERE id = %s
    #             """,
    #             (r.email, r.name, r.password, r.role, r.updated_ts, r.id)
    #         )
    #     conn.commit()
    #     cur.close()
    #     conn.close()

    # DELETE. 
    delete_ids = (
        # print('delete hit'),
        delete_df
        .select("before_id") # alias defined in df_flat above
        .dropna()
        .distinct()
        .collect()
    )

    # print("Records da eliminare:", delete_ids)

    if delete_ids:
        ids = [r.before_id for r in delete_ids]
        print(f"Deleting {len(ids)} records from employee_admin")

        # but only insert/update
        conn = psycopg2.connect(db_url) # Use  psycopg2 instead of JDBC because Spark JDBC doesn't suport DELETE function 
        cur = conn.cursor()
        cur.execute(
            "DELETE FROM employee_admin WHERE id = ANY(%s);",
            (ids,)
        )
        conn.commit()
        cur.close()
        conn.close()

# COMBINE valid_df and delete_df to pass both to write_to_postgres function
cdc_df = valid_df.unionByName(delete_df, allowMissingColumns=True)

query = (
    cdc_df # for every new inser/delete/update, we feed this data to write_to_postgres function that performs CRUD operations batches
    # in write_to_postgres db
    .writeStream
    .foreachBatch(write_to_postgres)
    .option("checkpointLocation", "/tmp_spark/checkpoint")
    .start()
)

query.awaitTermination()

# rm -rf /tmp_spark/checkpoint

# docker exec -it docker-kafka-1 kafka-topics --bootstrap-server localhost:9092 --list (there should be the topic cdc.public.Employee. if not kafka can crash)
# sometimes in dev you need to create it before starting spark:
# docker exec -it docker-kafka-1 kafka-topics --bootstrap-server localhost:9092 --create   --topic cdc.public.Employee   --partitions 1   --replication-factor 1
