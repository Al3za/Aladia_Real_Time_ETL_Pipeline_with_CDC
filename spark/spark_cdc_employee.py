import os
from urllib.parse import urlparse
from dotenv import load_dotenv
# Creare Spark session con supporto Kafka:
from pyspark.sql import SparkSession
import psycopg2 
from psycopg2.extras import execute_batch # per eseguire batch di dati in modo veloce e' sicuro

# Carica le variabili dal file
#load_dotenv("config/postgres.env")
load_dotenv()

db_url = os.environ["DATABASE_URL"]

parsed = urlparse(db_url)
port = parsed.port or 5432

jdbc_url = f"jdbc:postgresql://{parsed.hostname}:{port}{parsed.path}?sslmode=require" # jdbc_url → Spark sa dove connettersi
jdbc_properties = { # jdbc_properties → contiene user/password/driver
    "user": parsed.username,
    "password": parsed.password,
    "driver": "org.postgresql.Driver"
}

print("JDBC URL:", jdbc_url)


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
JARS_DIR = os.path.join(BASE_DIR, "jars")
print(JARS_DIR, 'JARS_DIR here')

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("CDC_Employee")
    .master("local[*]")
    .config(
        "spark.jars",
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/kafka-clients-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/spark-token-provider-kafka-0-10_2.12-3.4.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/commons-pool2-2.11.1.jar,"
        "/home/ale_linux/Aladia_Real_Time_ETL_Pipeline_with_CDC/jars/postgresql-42.6.0.jar"
    )
    .getOrCreate()
) 


spark.sparkContext.setLogLevel("WARN")
print("Spark ready! Version:", spark.version)


# Spark si connette a kafka tramite port 29092(localhost:29092, definito in docker-compose). ricorda che i topic sono i messaggi CDC salvati qui in kafka
# "cdc.public.Employee" e il prefisso dove si trovano i topics kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:29092") \
    .option("subscribe", "cdc.public.Employee") \
    .option("startingOffsets", "earliest") \
    .load()

#  HERE STARTS ETL: 
from pyspark.sql.functions import from_json, col, to_timestamp, expr, lit
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Estarai solo i dati che ci servono da payload
# Il messagio di kafka e' un 'mess'. qui estraiamo i dati che ci servono dal messagio kafka.
# Ricordiamoci di droppare le colonne op,
schema = StructType([
    StructField("payload", StructType([ # res #payload" (nel message kafka)
        StructField("op", StringType(), True), # in op is store: insert/update/delete operation type in topyc generated by Debezium
        StructField("after", StructType([ # valori nuovi(insert), o aggiornati(update)
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True),
         StructField("before", StructType([      # valori prima dell'update o delete
            StructField("id", LongType()),
            StructField("email", StringType()),
            StructField("name", StringType()),
            StructField("createdAt", LongType()),
            StructField("updatedAt", LongType()),
            StructField("role", StringType()),
            StructField("password", StringType())
        ]), True)
    ]), True),
     StructField("_corrupt_record", StringType(), True) # _corrupt_record = column name create be me where invalid json data (sended by kafka) ll be stored
]) # same types as the sink db table, to avoid errors


# parse the "payload" message data stored in kafka.

df_parsed = ( # df_parsed = data cleaning with only data defined in schema
    df.selectExpr("CAST(value AS STRING) as json_str")
      .select(from_json(
              col("json_str"), 
               schema,
                {"mode": "PERMISSIVE", 
                 "columnNameOfCorruptRecord": "_corrupt_record"})
                .alias("data"))
) #"PERMISSIVE": Spark salva i record malformati nella colonna _corrupt_record invece di far crashare il job spark.
  # _corrupt_record possono essere json malformati o senza values creati da kafka es: {emai:''} or {id:abc} 
  # questa colonna, insieme a invalid_df(i dati che non hanno passato il filtraggio sotto check("ADMIN", "ENGINEER", "INTERN"))  

# df_parsed.printSchema() # check lo schema

df_flat = df_parsed \
    .filter(col("data.payload").isNotNull()) \
    .select(
          col("data.payload.op").alias("op"), # qui attingiamo il campo "op" di debezium (insert/update/delete) topics
          col("data.payload.after.*"), # # values post-update/insert (di kafka)
          col("data.payload.before.id").alias("before_id"), # id values pre-update/delete )
          col("data._corrupt_record")  # column name where invalid/malformed records ll be stored
) # Le colonne di controllo CDC, es per efter:


# handle  malformed e valid json records(found errors list in tmp/badRecords_checkpoint)
df_malformed = df_flat.filter(col("_corrupt_record").isNotNull()) # record json dei kafka topic malformati(_corrupt_record, andranno in tmp/badRecords_checkpoint, non andranno in db){
#   "op": "d",
#   "before": { "id": 177, ... },
#   "after": null
# }

df_valid_json = df_flat.filter(col("_corrupt_record").isNull())

# separate delete functionality to access it in write_to_postgres function(down below)
delete_df = df_valid_json.filter(col("op") == "d")

# validation for create/update/read operations topics only
UpSert_and_Read_df = df_valid_json.filter(col("op").isin("c", "u", "r"))

# DATA VALIDATION

valid_df = UpSert_and_Read_df.filter( # checks that valid data ("c", "u", "r"), follow this structure. If not,  data ll stop here (not reach sink db)
    (col("role").isin("ADMIN", "ENGINEER","INTERN")) &
    (col("email").isNotNull()) &
    (col("password")).isNotNull() &  
    (col("createdAt") > 0)
) 

print('valid_df schema:', valid_df)

# gracefully handle invalid records without blocking spark stream 
invalid_df = df_valid_json.filter(
    (~col("role").isin("ADMIN", "ENGINEER", "INTERN")) |
    col("email").isNull() |
    (col("createdAt") <= 0)
) # data che non passa questo filtro, verra' inserito in badRecords_checkpoint file in 
  # formato parque, insieme hai json malformati di kafka

invalid_df = invalid_df.withColumn( # helpfull message you get in badRecords_checkpoint (parque format)
    "error_reason",
    expr("""
        CASE
            WHEN role NOT IN ('ADMIN','ENGINEER','INTERN') THEN 'INVALID_ROLE'
            WHEN email IS NULL THEN 'NULL_EMAIL'
            WHEN createdAt <= 0 THEN 'INVALID_CREATED_AT'
            ELSE 'UNKNOWN'
        END
    """)
) # this error will be shows this error if we investigate in /tmp/badRecords_checkpoint

# store malformed and invalid records in badRecords folder(for further analysis)
(
    df_malformed
    .unionByName(invalid_df, allowMissingColumns=True) # unifica due categorie diverse di errore: errori json della(colonna _corrupt_record), e "invalid_df"(non allowed roles, datetimes...)
    .writeStream
    .format("parquet")
    .option("path", "/tmp/badRecords")
    .option("checkpointLocation", "/tmp/badRecords_checkpoint")
    .outputMode("append")
    .start()
)

# FUNCTION TO WRITE BATCH TO POSTGRESQL
def write_to_postgres(batch_df, batch_id):

    if batch_df.isEmpty():
        return # return nothing

    insert_df   = batch_df.filter(col("op") == "c")
    update_df   = batch_df.filter(col("op") == "u")
    snapshot_df = batch_df.filter(col("op") == "r")
    delete_df   = batch_df.filter(col("op") == "d")
     # works tnx to delete_df data  defined in cdc_df(delete_df): (cdc_df = valid_df.unionByName(delete_df, allowMissingColumns=True))
    delete_df.select("op", "before_id", "id").show(truncate=False)
    print("DELETE DF COUNT:", delete_df.count())

    upsert_source_df = (
        insert_df
        .unionByName(update_df)
        .unionByName(snapshot_df)
    ) # Combiniamo insert + update + snapshot in un unico dataframe. cosi evitiamo code redundance

    upsert_df = (
        upsert_source_df # batch dara with where applies business logic and deduplication
        .filter(col("role") == "ADMIN") # Only ADMINs record for insert/update on destination db
        .withColumn("updated_ts", to_timestamp(col("updatedAt") / 1000)) # withColumn crea una colonna temporanea che possiamo usare come riferimento, come facciamo sotto
        .dropDuplicates(["id", "updated_ts"]) # si basa su payload "after" data.  
        # “Se Kafka mi manda due volte lo stesso evento CDC (stesso id + stesso timestamp), ne tengo solo uno.” (Perfetto per CDC Debezium, perche a volte crea lo stesso evento 2)
        # questo non conta per update, perche avremo lo stesso id, ma update_ts diverso
    )
        # DROP COLONNE CDC / TECNICHE PRIMA DEL WRITE. in cdc_df abbiamo non solo i datatype del db (email,name..) ma anche le
        # colonne CDC definite in "schema/". upsert_df trasorta tutti questi nel blocco codice sotto, dove in pratica facciamo gli insert/update
        # spark le confronta questi dati con la tabella "employee_admin" ma la' non risulta la colonna "op","before_id".. causando l'error
        # quindi le droppiamo per lasciare solo le colonne uguali allo schema del sink db 
    upsert_df = upsert_df.drop("op", "before_id", "_corrupt_record", "updated_ts") # cosi' restano solo le colonne che spark 
        # va' ad inserire nel sink db
   
    # --- PREPARE DATA ---
    upsert_rows = [
        (r.id, r.email, r.name, r.password, r.role, r.updatedAt)
        for r in upsert_df.select(
            "id", "email", "name", "password", "role", "updatedAt"
        ).collect()
    ]


    delete_ids = [
        r.before_id
        for r in delete_df.select("before_id").dropna().distinct().collect()
    ]

    print("===== DELETE IDS =====", delete_ids)

    if delete_ids:
       upsert_df = upsert_df.filter(~col("id").isin(delete_ids))

    if not upsert_rows and not delete_ids:
        return
 
    if upsert_rows or delete_ids: # 
        # in upsert_df arrivano i dati di cdc_df definiti in 'query'(sotto) quando chiamiamo
        #  questa funzione write_to_postgres

        # --- DB TRANSACTION (1 BATCH = 1 TRANSACTION) --- 
        # O il batch va in port per intero, ho tutti i dati vengono tornati indietro, 
        # e si ripete il job, senza salvare dati parziali nel db di destinazione, e neanche nel file checkpoint
        conn = psycopg2.connect(db_url)
        conn.autocommit = False
        cur = conn.cursor()

        try:
        # UPSERT (INSERT / UPDATE)
        # Costruiamo la query UPSERT. lo stack kafka /Spark/ jdbc con spesso fa' partire un error quando si fa' un update, 
        # perche jdbc tende a fare un insert con i dati dell update, causando un crash di duplicati del db. ecco perche UPSERT 
        # e' necessario. in questa query stiamo dicendo al db: 
        # “Se il record esiste già (stesso id), allora aggiorna i campi; altrimenti inseriscilo nuovo.”"
            if upsert_rows:
                  # Idempotenza garantita 
                  upsert_query = """
                    INSERT INTO employee_admin (id, email, name, password, role, updatedAt)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (id) DO UPDATE
                    SET email = EXCLUDED.email,
                        name = EXCLUDED.name,
                        password = EXCLUDED.password,
                        role = EXCLUDED.role,
                        updatedAt = EXCLUDED.updatedAt;
                   """ # nella upsert_query qui sopra, ON CONFLICT (id) DO UPDATE assicura Idempotenza garantita e 
                       # niente duplicati a causa di un crasch di spark e rilettura dei topic kafka gia processati ma non salvati nel file checkpoint
                  
                  execute_batch(cur, upsert_query, upsert_rows, page_size=100) # per eseguire batch di dati in modo veloce e' sicuro

        # DELETE. 
            if delete_ids:
               
               cur.execute(
                  "DELETE FROM employee_admin WHERE id = ANY(%s);",
                  (delete_ids,)
               )

        # ✅ COMMIT SOLO QUI(un solo commit per batch di upsert e delete). se spark crasha prima fare il commit, avviene il rollback, il checkpoint non si aggiorna, e retry sicuro.(100% Exactly once principle) 
            conn.commit()
            # print("Records da eliminare:", delete_ids)

            
        except Exception as e:
            # TUTTO ANNULLATO SE QUALCOSA VA STORTO
            conn.rollback()
            raise e # errore di exeption
        finally:
            cur.close()
            conn.close()

# COMBINE valid_df and delete_df to pass both to write_to_postgres function
cdc_df = valid_df.unionByName(delete_df, allowMissingColumns=True)

query = (
    cdc_df # for every new inser/delete/update, we feed this data to write_to_postgres function that performs CRUD operations batches
    # in write_to_postgres db
    .writeStream
    .foreachBatch(write_to_postgres) # foreachBatch ci permette logica custom (UPSERT / DELETE) e checkpoint gestito da Spark
    .option("checkpointLocation", "/tmp_spark/checkpoint") # Spark sa da dove ripartire. offset Kafka + stato streaming salvati (usa s3 in produzione)
    .start()
) # 

query.awaitTermination()

# rm -rf /tmp_spark/checkpoint

# docker exec -it docker-kafka-1 kafka-topics --bootstrap-server localhost:9092 --list (there should be the topic cdc.public.Employee. if not kafka can crash)
# sometimes in dev you need to create it before starting spark:
# docker exec -it docker-kafka-1 kafka-topics --bootstrap-server localhost:9092 --create   --topic cdc.public.Employee   --partitions 1   --replication-factor 1
